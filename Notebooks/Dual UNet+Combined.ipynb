{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw_-hFm6bjY6"
      },
      "source": [
        "## ğŸŒ Connect Colab to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2S4GWr3Uoa8",
        "outputId": "eb1aa02d-cc6b-4178-df4b-98dc2ed60356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n",
            "/gdrive/My Drive/[2024-2025] AN2DL Homework 2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/gdrive\")\n",
        "%cd /gdrive/My Drive\n",
        "%cd [2024-2025] AN2DL Homework 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7IqZP5Iblna"
      },
      "source": [
        "## âš™ï¸ Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CO6_Ft_8T56A"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "tfk.config.enable_unsafe_deserialization()\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras.layers import Layer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import mode\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 29\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN_cpHlSboXV"
      },
      "source": [
        "## â³ Load the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLaoDaG1V1Yg",
        "outputId": "58bc1327-48cb-4f7e-9349-f4b95a9f9a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training X shape: (2615, 64, 128)\n",
            "Training y shape: (2615, 64, 128)\n",
            "Test X shape: (10022, 64, 128)\n",
            "Input shape: (64, 128, 1)\n",
            "Number of classes: 5\n"
          ]
        }
      ],
      "source": [
        "data = np.load(\"mars_for_students.npz\")\n",
        "\n",
        "training_set = data[\"training_set\"]\n",
        "X_train = training_set[:, 0]\n",
        "y_train = training_set[:, 1]\n",
        "\n",
        "X_test = data[\"test_set\"]\n",
        "\n",
        "print(f\"Training X shape: {X_train.shape}\")\n",
        "print(f\"Training y shape: {y_train.shape}\")\n",
        "print(f\"Test X shape: {X_test.shape}\")\n",
        "\n",
        "# Add color channel and rescale pixels between 0 and 1\n",
        "X_train = X_train[..., np.newaxis] / 255.0\n",
        "X_test = X_test[..., np.newaxis] / 255.0\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3VPMx3wpqVd"
      },
      "source": [
        "## ğŸ” Inspect the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y5_2vtLTpufm",
        "outputId": "502936ff-e715-447d-e6f3-b566ac98dd59"
      },
      "outputs": [],
      "source": [
        "# Calculate prevalent labels\n",
        "y_train_labels = mode(y_train, axis=(1, 2))[0].flatten()\n",
        "\n",
        "print(f\"Shape X_train: {X_train.shape}\")\n",
        "print(f\"Shape y_train_labels: {y_train_labels.shape}\")\n",
        "\n",
        "# List all unique labels to check correctness\n",
        "unique_labels = np.unique(y_train)\n",
        "print(f\"Unique classes: {unique_labels}\")\n",
        "\n",
        "# Plot images in batches\n",
        "def plot_images(X, y, start_index=0, images_per_row=10, images_per_col=10):\n",
        "    fig, axes = plt.subplots(images_per_col, images_per_row, figsize=(15, 15))\n",
        "    for i in range(images_per_row * images_per_col):\n",
        "        idx = start_index + i\n",
        "        if idx >= len(X):\n",
        "            break\n",
        "        ax = axes[i // images_per_row, i % images_per_row]\n",
        "        ax.imshow(X[idx], cmap=\"gray\")\n",
        "        ax.set_title(f\"Class: {y[idx]}\")\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot a sample image from each class\n",
        "def plot_one_sample_per_class(X, y, y_mask, classes):\n",
        "    for label in classes:\n",
        "        for i in range(len(y_mask)):\n",
        "            if label in np.unique(y_mask[i]):\n",
        "                plt.figure()\n",
        "                plt.imshow(X[i], cmap=\"gray\")\n",
        "                plt.title(f\"Class: {label}\")\n",
        "                plt.axis(\"off\")\n",
        "                plt.show()\n",
        "                break\n",
        "\n",
        "plot_one_sample_per_class(X_train, y_train_labels, y_train, unique_labels)\n",
        "\n",
        "# Plot all images\n",
        "images_per_row = 10\n",
        "images_per_col = 10\n",
        "images_per_page = images_per_row * images_per_col\n",
        "num_images = X_train.shape[0]\n",
        "\n",
        "for start_idx in range(0, num_images, images_per_page):\n",
        "    plot_images(X_train, y_train_labels, start_index=start_idx, images_per_row=images_per_row, images_per_col=images_per_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z6VS98FeMBD"
      },
      "source": [
        "## âŒ Remove outliers from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PzVZbyNco6v",
        "outputId": "28c352ea-1f2c-4f9d-9b4e-cd648d3b5586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape X_train_filtered: (2505, 64, 128, 1)\n",
            "Shape y_train_filtered: (2505, 64, 128)\n",
            "Unique classes: [0. 1. 2. 3. 4.]\n"
          ]
        }
      ],
      "source": [
        "# Lists to contain filtered elements\n",
        "X_train_filtered = []\n",
        "y_train_filtered = []\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    label = y_train[i].argmax() if y_train.ndim > 1 else y_train[i]\n",
        "    if label != 415:\n",
        "        # Add to filtered dataset the non-alien images\n",
        "        X_train_filtered.append(X_train[i])\n",
        "        y_train_filtered.append(y_train[i])\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train_filtered = np.array(X_train_filtered)\n",
        "y_train_filtered = np.array(y_train_filtered)\n",
        "\n",
        "print(f\"Shape X_train_filtered: {X_train_filtered.shape}\")\n",
        "print(f\"Shape y_train_filtered: {y_train_filtered.shape}\")\n",
        "print(f\"Unique classes: {np.unique(y_train_filtered)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fE76Lu-Ea0-"
      },
      "source": [
        "## ğŸ” Inspect the filtered training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ibagIaQkHRiK",
        "outputId": "791cda09-aefa-43f4-c7a5-e7da21f103bf"
      },
      "outputs": [],
      "source": [
        "num_images_filtered = X_train_filtered.shape[0]\n",
        "y_train_filtered_labels = mode(y_train_filtered, axis=(1, 2))[0].flatten()\n",
        "\n",
        "# Plot the filtered dataset\n",
        "for start_idx in range(0, num_images_filtered, images_per_page):\n",
        "    plot_images(X_train_filtered, y_train_filtered_labels, start_index=start_idx, images_per_row=images_per_row, images_per_col=images_per_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsXRC_eIlqdY"
      },
      "source": [
        "## ğŸ§® Define network parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB8MXxkwltdh"
      },
      "outputs": [],
      "source": [
        "# Set batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# Set learning rate for the optimizer\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Set early stopping patience threshold\n",
        "patience = 10\n",
        "\n",
        "# Set maximum number of training epochs\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEb5t0AgmfQc"
      },
      "source": [
        "## âœ‚ Split into Training and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVf9fnTumtuP",
        "outputId": "bc29103d-4b13-4072-9e41-0ef5c6e5a3b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape:\t (2004, 64, 128, 1) (2004, 64, 128)\n",
            "Validation set shape:\t (501, 64, 128, 1) (501, 64, 128)\n"
          ]
        }
      ],
      "source": [
        "# Split the training dataset to get a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_filtered,\n",
        "    y_train_filtered,\n",
        "    test_size=0.2,\n",
        "    random_state=seed)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print('Training set shape:\\t', X_train.shape, y_train.shape)\n",
        "print('Validation set shape:\\t', X_val.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVztd7_HgvOq"
      },
      "source": [
        "## ğŸ”„ Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_laNhjCPgxQo"
      },
      "outputs": [],
      "source": [
        "def augment_data(image, label):\n",
        "    # Geometric Transformations\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    label = tf.image.random_flip_left_right(label)\n",
        "\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    label = tf.image.random_flip_up_down(label)\n",
        "\n",
        "    # Chromatic Transformations\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = tf.expand_dims(image, axis=-1) if len(image.shape) == 2 else image\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    return image\n",
        "\n",
        "def preprocess_label(label):\n",
        "    label = tf.expand_dims(label, axis=-1) if len(label.shape) == 2 else label\n",
        "    label = tf.cast(label, tf.int32)\n",
        "    return label\n",
        "\n",
        "def preprocess_data(image, label):\n",
        "    image = preprocess_image(image)\n",
        "    label = preprocess_label(label)\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnFLkc1chG53"
      },
      "outputs": [],
      "source": [
        "# Original dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Augmented dataset\n",
        "augmented_dataset = train_dataset.map(lambda x, y: augment_data(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Combined dataset, having both augmented and original dataset\n",
        "combined_dataset = train_dataset.concatenate(augmented_dataset)\n",
        "combined_dataset = combined_dataset.shuffle(buffer_size=len(X_train))\n",
        "combined_dataset = combined_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzyhdlxCF2-X"
      },
      "source": [
        "## ğŸ” Plot Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "BIprDuBhF3-p",
        "outputId": "692913ee-e0f8-46ff-d9f1-cf9c397420e9"
      },
      "outputs": [],
      "source": [
        "def plot_from_dataset(dataset, title, num_images=10):\n",
        "    # Imposta il layout orizzontale\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 3))\n",
        "    fig.suptitle(title, fontsize=16, y=1.05)\n",
        "    count = 0\n",
        "\n",
        "    for batch in dataset:\n",
        "        images, label_maps = batch  # Estrai immagini e feature map\n",
        "        for i in range(len(images)):\n",
        "            if count >= num_images:\n",
        "                break\n",
        "\n",
        "            # Prepara l'immagine\n",
        "            image = images[i].numpy()\n",
        "            if image.shape[-1] == 1:  # Scala di grigi\n",
        "                image = tf.squeeze(image, axis=-1).numpy()\n",
        "\n",
        "            # Mostra l'immagine\n",
        "            axes[count].imshow(image, cmap='gray' if image.ndim == 2 else None, aspect='auto')\n",
        "            axes[count].axis('off')\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        if count >= num_images:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_from_dataset(train_dataset, \"Train Dataset without Augmentation\", num_images=10)\n",
        "plot_from_dataset(augmented_dataset, \"Train Dataset with Augmentation\", num_images=10)\n",
        "plot_from_dataset(combined_dataset, \"Combined Dataset\", num_images=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUpegWw8SLNr"
      },
      "source": [
        "## ğŸ”¨ Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0EtfNnxhXTQ"
      },
      "outputs": [],
      "source": [
        "class ResizeLayer(Layer):\n",
        "    def __init__(self, target_size, **kwargs):\n",
        "        super(ResizeLayer, self).__init__(**kwargs)\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.image.resize(inputs, self.target_size)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.target_size[0], self.target_size[1], input_shape[-1])\n",
        "\n",
        "class AdaptShapeLayer(tfkl.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AdaptShapeLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, tensor, target_tensor):\n",
        "        target_shape = tf.shape(target_tensor)[1:3]\n",
        "        tensor = tf.image.resize(tensor, target_shape, method='bilinear')\n",
        "        return tensor\n",
        "\n",
        "def adapt_channels(tensor, target_channels):\n",
        "    conv = tfkl.Conv2D(target_channels, kernel_size=1, padding='same')\n",
        "    return conv(tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK5V0Oyf3Ec7"
      },
      "outputs": [],
      "source": [
        "def unet_block(input_tensor, filters, kernel_size=3, stack=4, name=''):\n",
        "    x = input_tensor\n",
        "    adapt_shape = AdaptShapeLayer()\n",
        "    residual = adapt_channels(input_tensor, filters)\n",
        "\n",
        "    for i in range(stack):\n",
        "        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = tfkl.BatchNormalization()(x)\n",
        "        x = se_block(x)\n",
        "        x = tfkl.Activation(tf.nn.leaky_relu)(x)\n",
        "        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = tfkl.BatchNormalization()(x)\n",
        "        x = se_block(x)\n",
        "        x = tfkl.Activation(tf.nn.leaky_relu)(x)\n",
        "        x = tfkl.SpatialDropout2D(0.2)(x)\n",
        "\n",
        "    residual = adapt_shape(residual, x)\n",
        "    x = tfkl.Add()([x, residual])\n",
        "    return x\n",
        "\n",
        "def dense_block(input_tensor, filters, kernel_size=3, growth_rate=32, num_layers=4):\n",
        "    x = input_tensor\n",
        "    for i in range(num_layers):\n",
        "        conv = tfkl.Conv2D(growth_rate, kernel_size, padding='same', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "        conv = tfkl.BatchNormalization()(conv)\n",
        "        conv = tfkl.Activation(tf.nn.leaky_relu)(conv)\n",
        "        x = tfkl.Concatenate()([x, conv])\n",
        "    return x\n",
        "\n",
        "def se_block(input_tensor, reduction_ratio=16):\n",
        "    filters = input_tensor.shape[-1]\n",
        "    se = tfkl.GlobalAveragePooling2D()(input_tensor)\n",
        "    se = tfkl.Dense(filters // reduction_ratio, activation='relu', kernel_initializer='he_normal')(se)\n",
        "    se = tfkl.Dense(filters, activation='sigmoid', kernel_initializer='he_normal')(se)\n",
        "    se = tfkl.Reshape((1, 1, filters))(se)\n",
        "    return tfkl.Multiply()([input_tensor, se])\n",
        "\n",
        "def refinement_block(x, filters):\n",
        "    for _ in range(2):\n",
        "        x = tfkl.Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "        x = tfkl.BatchNormalization()(x)\n",
        "    return x\n",
        "\n",
        "# Parallel Dilated Convolutions\n",
        "def par_dil_conv(input_tensor, filters, kernel_size=3, dilation_rates=(1, 2, 4)):\n",
        "    branches = []\n",
        "    for rate in dilation_rates:\n",
        "        branch = tfkl.Conv2D(filters, kernel_size=kernel_size, dilation_rate=rate, padding='same', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(input_tensor)\n",
        "        branch = tfkl.BatchNormalization()(branch)\n",
        "        branch = tfkl.Activation(tf.nn.leaky_relu)(branch)\n",
        "        branches.append(branch)\n",
        "\n",
        "    output = tfkl.Concatenate()(branches)\n",
        "    return output\n",
        "\n",
        "# Atrous Spatial Pyramid Pooling\n",
        "def aspp(input_tensor, filters):\n",
        "    branch1 = tfkl.Conv2D(filters, kernel_size=1, padding='same', activation='relu')(input_tensor)\n",
        "    branch2 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=6, padding='same', activation='relu')(input_tensor)\n",
        "    branch3 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=12, padding='same', activation='relu')(input_tensor)\n",
        "    branch4 = tfkl.Conv2D(filters, kernel_size=3, dilation_rate=18, padding='same', activation='relu')(input_tensor)\n",
        "\n",
        "    # Pooling branch\n",
        "    pooling = tfkl.GlobalAveragePooling2D()(input_tensor)\n",
        "    pooling = tfkl.Reshape((1, 1, input_tensor.shape[-1]))(pooling)\n",
        "    pooling = tfkl.Conv2D(filters, kernel_size=1, padding='same', activation='relu')(pooling)\n",
        "    pooling = tfkl.UpSampling2D(size=(input_tensor.shape[1], input_tensor.shape[2]))(pooling)\n",
        "\n",
        "    output = tfkl.Concatenate()([branch1, branch2, branch3, branch4, pooling])\n",
        "    return tfkl.Conv2D(filters, kernel_size=1, padding='same', activation='relu')(output)\n",
        "\n",
        "\n",
        "def bottleneck_layer(input_tensor, filters, reduction_ratio=4, dilation_rates=(1, 2, 4)):\n",
        "    adapt_shape = AdaptShapeLayer()\n",
        "\n",
        "    residual = adapt_channels(input_tensor, filters)\n",
        "\n",
        "    # Bottleneck Structure\n",
        "    reduced_filters = filters // reduction_ratio\n",
        "    x = tfkl.Conv2D(reduced_filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(input_tensor)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.SeparableConv2D(filters, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = par_dil_conv(x, filters=filters, kernel_size=3, dilation_rates=dilation_rates)\n",
        "    x = tfkl.Conv2D(filters, kernel_size=1, padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = se_block(x)\n",
        "    x = tfkl.Dropout(0.3)(x)\n",
        "    residual = adapt_shape(residual, x)\n",
        "    x = tfkl.Add()([x, residual])\n",
        "    return x\n",
        "\n",
        "def refinement_block(x, filters):\n",
        "    x = tfkl.Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCVur8U_3FWN"
      },
      "source": [
        "## ğŸ”¨ Build UNets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH96h-n22oOo"
      },
      "outputs": [],
      "source": [
        "def create_global_unet(input_shape, num_classes, dilation_rates):\n",
        "    inputs = tfkl.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x1 = unet_block(inputs, filters=8, name='global_enc1')\n",
        "    p1 = tfkl.MaxPooling2D((2, 2))(x1)\n",
        "    x2 = unet_block(p1, filters=16, name='global_enc2')\n",
        "    p2 = tfkl.MaxPooling2D((2, 2))(x2)\n",
        "    x3 = unet_block(p2, filters=32, name='global_enc3')\n",
        "    p3 = tfkl.MaxPooling2D((2, 2))(x3)\n",
        "    x4 = unet_block(p3, filters=64, name='global_enc4')\n",
        "    p4 = tfkl.MaxPooling2D((2, 2))(x4)\n",
        "\n",
        "    # Bottleneck\n",
        "    b = bottleneck_layer(p4, filters=128, dilation_rates=dilation_rates)\n",
        "    b = aspp(b, filters=256)  # ASPP\n",
        "\n",
        "    # Decoder\n",
        "    u4 = tfkl.UpSampling2D((2, 2))(b)\n",
        "    c4 = tfkl.Concatenate()([u4, x4])\n",
        "    d4 = refinement_block(c4, filters=64)\n",
        "    u3 = tfkl.UpSampling2D((2, 2))(d4)\n",
        "    c3 = tfkl.Concatenate()([u3, x3])\n",
        "    d3 = refinement_block(c3, filters=32)\n",
        "    u2 = tfkl.UpSampling2D((2, 2))(d3)\n",
        "    c2 = tfkl.Concatenate()([u2, x2])\n",
        "    d2 = refinement_block(c2, filters=16)\n",
        "    u1 = tfkl.UpSampling2D((2, 2))(d2)\n",
        "    c1 = tfkl.Concatenate()([u1, x1])\n",
        "    d1 = refinement_block(c1, filters=8)\n",
        "\n",
        "    outputs = tfkl.Conv2D(num_classes, kernel_size=1, activation='softmax')(d1)\n",
        "\n",
        "    model = tfk.Model(inputs=inputs, outputs=outputs, name=\"Global_U-Net\")\n",
        "    return model\n",
        "\n",
        "def create_local_unet(input_shape, num_classes, dilation_rates):\n",
        "    inputs = tfkl.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x1 = unet_block(inputs, filters=8, name='local_enc1')\n",
        "    p1 = tfkl.MaxPooling2D((2, 2))(x1)\n",
        "    x2 = unet_block(p1, filters=16, name='local_enc2')\n",
        "    p2 = tfkl.MaxPooling2D((2, 2))(x2)\n",
        "    x3 = unet_block(p2, filters=32, name='local_enc3')\n",
        "    p3 = tfkl.MaxPooling2D((2, 2))(x3)\n",
        "    x4 = unet_block(p3, filters=64, name='local_enc4')\n",
        "    p4 = tfkl.MaxPooling2D((2, 2))(x4)\n",
        "\n",
        "    # Bottleneck\n",
        "    b = bottleneck_layer(p4, filters=128, dilation_rates=dilation_rates)\n",
        "    b = par_dil_conv(b, filters=256, kernel_size=3, dilation_rates=(1,2,4))\n",
        "\n",
        "    # Decoder\n",
        "    u4 = tfkl.UpSampling2D((2, 2))(b)\n",
        "    c4 = tfkl.Concatenate()([u4, x4])\n",
        "    d4 = refinement_block(c4, filters=64)\n",
        "    u3 = tfkl.UpSampling2D((2, 2))(d4)\n",
        "    c3 = tfkl.Concatenate()([u3, x3])\n",
        "    d3 = refinement_block(c3, filters=32)\n",
        "    u2 = tfkl.UpSampling2D((2, 2))(d3)\n",
        "    c2 = tfkl.Concatenate()([u2, x2])\n",
        "    d2 = refinement_block(c2, filters=16)\n",
        "    u1 = tfkl.UpSampling2D((2, 2))(d2)\n",
        "    c1 = tfkl.Concatenate()([u1, x1])\n",
        "    d1 = refinement_block(c1, filters=8)\n",
        "\n",
        "    outputs = tfkl.Conv2D(num_classes, kernel_size=1, activation='softmax')(d1)\n",
        "\n",
        "    model = tfk.Model(inputs=inputs, outputs=outputs, name=\"Local_U-Net\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e2Mz9gUVbGd"
      },
      "outputs": [],
      "source": [
        "# Dice Loss\n",
        "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes, axis=-1)\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    dice = (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
        "    return 1 - dice\n",
        "\n",
        "# Focal Loss\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    gamma = tf.constant(gamma, dtype=tf.float32)\n",
        "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=y_pred.shape[-1])\n",
        "        y_true_one_hot = tf.squeeze(y_true_one_hot, axis=-2) if len(y_true_one_hot.shape) > len(y_pred.shape) else y_true_one_hot\n",
        "        cross_entropy = -y_true_one_hot * tf.keras.backend.log(y_pred)\n",
        "        weight = alpha * tf.math.pow((1 - y_pred), gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# Combined Loss\n",
        "def combined_loss(y_true, y_pred, dice_weight=0.5, focal_weight=0.5):\n",
        "    dice = dice_loss(y_true, y_pred, smooth=1e-6)\n",
        "    focal = focal_loss(gamma=2.0, alpha=0.25)(y_true, y_pred)\n",
        "    combined = dice_weight * dice + focal_weight * focal\n",
        "\n",
        "    return combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "CBkb3TRF1KJx",
        "outputId": "8aca1c1d-3307-46ac-d7eb-6f9e5c4f67b9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Double_U-Net\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"Double_U-Net\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)              </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">        Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to           </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_5             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ Global_U-Net (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">74,568,741</span> â”‚ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ Local_U-Net (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">76,078,373</span> â”‚ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ concatenate_13            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)    â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ Global_U-Net[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             â”‚                        â”‚                â”‚ Local_U-Net[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_152 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">5,824</span> â”‚ concatenate_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_124   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_152[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_153 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> â”‚ batch_normalization_1â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_154 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)     â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> â”‚ conv2d_153[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_5             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)     â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ Global_U-Net (\u001b[38;5;33mFunctional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m5\u001b[0m)     â”‚     \u001b[38;5;34m74,568,741\u001b[0m â”‚ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ Local_U-Net (\u001b[38;5;33mFunctional\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m5\u001b[0m)     â”‚     \u001b[38;5;34m76,078,373\u001b[0m â”‚ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ concatenate_13            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m10\u001b[0m)    â”‚              \u001b[38;5;34m0\u001b[0m â”‚ Global_U-Net[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    â”‚\n",
              "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)             â”‚                        â”‚                â”‚ Local_U-Net[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_152 (\u001b[38;5;33mConv2D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    â”‚          \u001b[38;5;34m5,824\u001b[0m â”‚ concatenate_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_124   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    â”‚            \u001b[38;5;34m256\u001b[0m â”‚ conv2d_152[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)      â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_153 (\u001b[38;5;33mConv2D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    â”‚         \u001b[38;5;34m36,928\u001b[0m â”‚ batch_normalization_1â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_154 (\u001b[38;5;33mConv2D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m5\u001b[0m)     â”‚            \u001b[38;5;34m325\u001b[0m â”‚ conv2d_153[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,690,447</span> (574.84 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m150,690,447\u001b[0m (574.84 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,632,719</span> (574.62 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,632,719\u001b[0m (574.62 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,728</span> (225.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m57,728\u001b[0m (225.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "inputs = tfkl.Input(shape=input_shape)\n",
        "\n",
        "# Crea le due reti separatamente\n",
        "global_unet = create_global_unet(input_shape, num_classes, (2, 4, 8))\n",
        "local_unet = create_local_unet(input_shape, num_classes, (1, 2, 4))\n",
        "\n",
        "# Input condiviso\n",
        "inputs = tfkl.Input(shape=input_shape)\n",
        "\n",
        "# Passaggio dell'input alle due reti\n",
        "global_output = global_unet(inputs)\n",
        "local_output = local_unet(inputs)\n",
        "\n",
        "# Fusione dei risultati (concatenazione)\n",
        "merged = tfkl.Concatenate()([global_output, local_output])\n",
        "\n",
        "# Un piccolo blocco di affinamento finale\n",
        "fused = refinement_block(merged, filters=64)\n",
        "\n",
        "# Output finale\n",
        "final_output = tfkl.Conv2D(num_classes, kernel_size=1, activation='softmax')(fused)\n",
        "\n",
        "model = tfk.Model(inputs=inputs, outputs=final_output)\n",
        "\n",
        "# Define the MeanIoU ignoring the background class\n",
        "mean_iou = tfk.metrics.MeanIoU(num_classes=num_classes, ignore_class=0, sparse_y_pred=False, name='mean_iou')\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tfk.optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "loss = combined_loss\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=[mean_iou]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSliIxBvbs2Q"
      },
      "source": [
        "## ğŸ› ï¸ Train and Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stfBHdtb6kZa"
      },
      "outputs": [],
      "source": [
        "# Create an EarlyStopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Create a LearningRate Scheduler\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_mean_iou', factor=0.5, patience=5, min_lr=1e-5\n",
        ")\n",
        "\n",
        "# Store the callbacks in a list\n",
        "callbacks = [early_stopping, lr_scheduler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pMCbSMQ_-XoH",
        "outputId": "d88a08f1-bbff-4fb7-a875-04a9064767f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 888ms/step - loss: 0.8645 - mean_iou: 0.1116 - val_loss: 0.4745 - val_mean_iou: 0.0939 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 137ms/step - loss: 0.4703 - mean_iou: 0.1202 - val_loss: 0.4427 - val_mean_iou: 0.1046 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.4426 - mean_iou: 0.1352 - val_loss: 0.4782 - val_mean_iou: 0.0768 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 131ms/step - loss: 0.4207 - mean_iou: 0.1703 - val_loss: 0.4188 - val_mean_iou: 0.1762 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 133ms/step - loss: 0.4062 - mean_iou: 0.2066 - val_loss: 0.3977 - val_mean_iou: 0.2392 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 137ms/step - loss: 0.3778 - mean_iou: 0.2523 - val_loss: 0.3622 - val_mean_iou: 0.2653 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.3544 - mean_iou: 0.2922 - val_loss: 0.3758 - val_mean_iou: 0.2222 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - loss: 0.3521 - mean_iou: 0.2833 - val_loss: 0.3356 - val_mean_iou: 0.2969 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - loss: 0.3261 - mean_iou: 0.3152 - val_loss: 0.2910 - val_mean_iou: 0.3905 - learning_rate: 5.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - loss: 0.3201 - mean_iou: 0.3342 - val_loss: 0.3081 - val_mean_iou: 0.3188 - learning_rate: 5.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 138ms/step - loss: 0.3213 - mean_iou: 0.3158 - val_loss: 0.2835 - val_mean_iou: 0.3777 - learning_rate: 5.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.3128 - mean_iou: 0.3374 - val_loss: 0.3248 - val_mean_iou: 0.3235 - learning_rate: 5.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - loss: 0.3151 - mean_iou: 0.3312 - val_loss: 0.3117 - val_mean_iou: 0.3463 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - loss: 0.3045 - mean_iou: 0.3497 - val_loss: 0.2742 - val_mean_iou: 0.3791 - learning_rate: 2.5000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2949 - mean_iou: 0.3552 - val_loss: 0.2767 - val_mean_iou: 0.3616 - learning_rate: 2.5000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - loss: 0.2998 - mean_iou: 0.3537 - val_loss: 0.2866 - val_mean_iou: 0.3781 - learning_rate: 2.5000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - loss: 0.2914 - mean_iou: 0.3622 - val_loss: 0.2840 - val_mean_iou: 0.3339 - learning_rate: 2.5000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.2960 - mean_iou: 0.3563 - val_loss: 0.2645 - val_mean_iou: 0.4099 - learning_rate: 2.5000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - loss: 0.2915 - mean_iou: 0.3709 - val_loss: 0.2586 - val_mean_iou: 0.4005 - learning_rate: 1.2500e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 133ms/step - loss: 0.2920 - mean_iou: 0.3639 - val_loss: 0.2508 - val_mean_iou: 0.4160 - learning_rate: 1.2500e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.2836 - mean_iou: 0.3787 - val_loss: 0.2582 - val_mean_iou: 0.4005 - learning_rate: 1.2500e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.2829 - mean_iou: 0.3767 - val_loss: 0.2739 - val_mean_iou: 0.3867 - learning_rate: 1.2500e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.2766 - mean_iou: 0.3898 - val_loss: 0.2495 - val_mean_iou: 0.4249 - learning_rate: 1.2500e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2818 - mean_iou: 0.3862 - val_loss: 0.2512 - val_mean_iou: 0.4219 - learning_rate: 6.2500e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2723 - mean_iou: 0.3874 - val_loss: 0.2475 - val_mean_iou: 0.4134 - learning_rate: 6.2500e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 138ms/step - loss: 0.2711 - mean_iou: 0.3945 - val_loss: 0.2447 - val_mean_iou: 0.4272 - learning_rate: 6.2500e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 137ms/step - loss: 0.2721 - mean_iou: 0.3946 - val_loss: 0.2483 - val_mean_iou: 0.4222 - learning_rate: 6.2500e-05\n",
            "Epoch 28/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - loss: 0.2736 - mean_iou: 0.3871 - val_loss: 0.2442 - val_mean_iou: 0.4271 - learning_rate: 6.2500e-05\n",
            "Epoch 29/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 131ms/step - loss: 0.2741 - mean_iou: 0.3931 - val_loss: 0.2431 - val_mean_iou: 0.4191 - learning_rate: 3.1250e-05\n",
            "Epoch 30/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.2659 - mean_iou: 0.3976 - val_loss: 0.2415 - val_mean_iou: 0.4340 - learning_rate: 3.1250e-05\n",
            "Epoch 31/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 138ms/step - loss: 0.2730 - mean_iou: 0.3866 - val_loss: 0.2401 - val_mean_iou: 0.4322 - learning_rate: 3.1250e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.2643 - mean_iou: 0.4076 - val_loss: 0.2396 - val_mean_iou: 0.4258 - learning_rate: 3.1250e-05\n",
            "Epoch 33/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2673 - mean_iou: 0.3952 - val_loss: 0.2397 - val_mean_iou: 0.4375 - learning_rate: 3.1250e-05\n",
            "Epoch 34/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2668 - mean_iou: 0.3960 - val_loss: 0.2398 - val_mean_iou: 0.4314 - learning_rate: 1.5625e-05\n",
            "Epoch 35/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - loss: 0.2613 - mean_iou: 0.4023 - val_loss: 0.2380 - val_mean_iou: 0.4368 - learning_rate: 1.5625e-05\n",
            "Epoch 36/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - loss: 0.2621 - mean_iou: 0.4067 - val_loss: 0.2383 - val_mean_iou: 0.4339 - learning_rate: 1.5625e-05\n",
            "Epoch 37/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - loss: 0.2646 - mean_iou: 0.3928 - val_loss: 0.2390 - val_mean_iou: 0.4337 - learning_rate: 1.5625e-05\n",
            "Epoch 38/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2647 - mean_iou: 0.4009 - val_loss: 0.2392 - val_mean_iou: 0.4364 - learning_rate: 1.5625e-05\n",
            "Epoch 39/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2606 - mean_iou: 0.4118 - val_loss: 0.2389 - val_mean_iou: 0.4320 - learning_rate: 1.0000e-05\n",
            "Epoch 40/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - loss: 0.2658 - mean_iou: 0.4037 - val_loss: 0.2379 - val_mean_iou: 0.4351 - learning_rate: 1.0000e-05\n",
            "Epoch 41/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - loss: 0.2622 - mean_iou: 0.3988 - val_loss: 0.2371 - val_mean_iou: 0.4372 - learning_rate: 1.0000e-05\n",
            "Epoch 42/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 137ms/step - loss: 0.2597 - mean_iou: 0.4013 - val_loss: 0.2365 - val_mean_iou: 0.4384 - learning_rate: 1.0000e-05\n",
            "Epoch 43/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2629 - mean_iou: 0.4030 - val_loss: 0.2380 - val_mean_iou: 0.4322 - learning_rate: 1.0000e-05\n",
            "Epoch 44/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2653 - mean_iou: 0.4058 - val_loss: 0.2377 - val_mean_iou: 0.4346 - learning_rate: 1.0000e-05\n",
            "Epoch 45/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - loss: 0.2665 - mean_iou: 0.4014 - val_loss: 0.2376 - val_mean_iou: 0.4325 - learning_rate: 1.0000e-05\n",
            "Epoch 46/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - loss: 0.2670 - mean_iou: 0.3955 - val_loss: 0.2368 - val_mean_iou: 0.4373 - learning_rate: 1.0000e-05\n",
            "Epoch 47/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 138ms/step - loss: 0.2580 - mean_iou: 0.4073 - val_loss: 0.2363 - val_mean_iou: 0.4394 - learning_rate: 1.0000e-05\n",
            "Epoch 48/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2577 - mean_iou: 0.4082 - val_loss: 0.2367 - val_mean_iou: 0.4353 - learning_rate: 1.0000e-05\n",
            "Epoch 49/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - loss: 0.2672 - mean_iou: 0.4018 - val_loss: 0.2370 - val_mean_iou: 0.4355 - learning_rate: 1.0000e-05\n",
            "Epoch 50/100\n",
            "\u001b[1m251/251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - loss: 0.2627 - mean_iou: 0.4046 - val_loss: 0.2366 - val_mean_iou: 0.4371 - learning_rate: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    combined_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=callbacks\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtM0ubgdOzG-"
      },
      "outputs": [],
      "source": [
        "timestep_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "model_filename = f\"model_{timestep_str}.keras\"\n",
        "model.save(model_filename)\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNp6pUZuddqC"
      },
      "source": [
        "## ğŸ“Š Test the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU00iEFcYi_X",
        "outputId": "6451e379-7b48-48d5-868f-bdfdea247346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m314/314\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 57ms/step\n",
            "Predictions shape: (10022, 64, 128)\n"
          ]
        }
      ],
      "source": [
        "model = tfk.models.load_model(model_filename, custom_objects={\n",
        "        \"ResizeLayer\": ResizeLayer,\n",
        "        'dice_loss': dice_loss,\n",
        "        'focal_loss': focal_loss,\n",
        "        'combined_loss': combined_loss,\n",
        "        'unet_block': unet_block,\n",
        "        'dense_block': dense_block,\n",
        "        'par_dil_conv': par_dil_conv,\n",
        "        'bottleneck_layer': bottleneck_layer,\n",
        "        'se_block': se_block\n",
        "    }\n",
        ")\n",
        "\n",
        "preds = model.predict(X_test)\n",
        "preds = np.argmax(preds, axis=-1)\n",
        "print(f\"Predictions shape: {preds.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KErlLGwOTsX6"
      },
      "source": [
        "## ğŸ’¾ Save the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPjMEKqZW5jX"
      },
      "outputs": [],
      "source": [
        "def y_to_df(y) -> pd.DataFrame:\n",
        "    n_samples = len(y)\n",
        "    y_flat = y.reshape(n_samples, -1)\n",
        "    df = pd.DataFrame(y_flat)\n",
        "    df[\"id\"] = np.arange(n_samples)\n",
        "    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
        "    return df[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s18kX1uDconq"
      },
      "outputs": [],
      "source": [
        "# Create the csv submission file\n",
        "timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n",
        "submission_filename = f\"submission_{timestep_str}.csv\"\n",
        "submission_df = y_to_df(preds)\n",
        "submission_df.to_csv(submission_filename, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dw_-hFm6bjY6",
        "d7IqZP5Iblna",
        "GN_cpHlSboXV",
        "A3VPMx3wpqVd",
        "2z6VS98FeMBD",
        "9fE76Lu-Ea0-",
        "dsXRC_eIlqdY",
        "KEb5t0AgmfQc",
        "OVztd7_HgvOq",
        "PzyhdlxCF2-X",
        "vUpegWw8SLNr",
        "RNp6pUZuddqC",
        "KErlLGwOTsX6"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
