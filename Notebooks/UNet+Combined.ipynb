{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw_-hFm6bjY6"
      },
      "source": [
        "## ğŸŒ Connect Colab to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2S4GWr3Uoa8",
        "outputId": "79292de2-f76d-4259-c3fe-91043ec6648d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n",
            "/gdrive/My Drive/[2024-2025] AN2DL Homework 2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/gdrive\")\n",
        "%cd /gdrive/My Drive\n",
        "%cd [2024-2025] AN2DL Homework 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7IqZP5Iblna"
      },
      "source": [
        "## âš™ï¸ Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CO6_Ft_8T56A"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "tfk.config.enable_unsafe_deserialization()\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from scipy.stats import mode\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 29\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN_cpHlSboXV"
      },
      "source": [
        "## â³ Load the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLaoDaG1V1Yg",
        "outputId": "6a9b016e-dabf-4fde-8ec7-11733440eedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training X shape: (2615, 64, 128)\n",
            "Training y shape: (2615, 64, 128)\n",
            "Test X shape: (10022, 64, 128)\n",
            "Input shape: (64, 128, 1)\n",
            "Number of classes: 5\n"
          ]
        }
      ],
      "source": [
        "data = np.load(\"mars_for_students.npz\")\n",
        "\n",
        "training_set = data[\"training_set\"]\n",
        "X_train = training_set[:, 0]\n",
        "y_train = training_set[:, 1]\n",
        "\n",
        "X_test = data[\"test_set\"]\n",
        "\n",
        "print(f\"Training X shape: {X_train.shape}\")\n",
        "print(f\"Training y shape: {y_train.shape}\")\n",
        "print(f\"Test X shape: {X_test.shape}\")\n",
        "\n",
        "# Add color channel and rescale pixels between 0 and 1\n",
        "X_train = X_train[..., np.newaxis] / 255.0\n",
        "X_test = X_test[..., np.newaxis] / 255.0\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3VPMx3wpqVd"
      },
      "source": [
        "## ğŸ” Inspect the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y5_2vtLTpufm",
        "outputId": "8d8d9d42-0a4e-491c-e347-cdd1b3d4c25b"
      },
      "outputs": [],
      "source": [
        "# Calculate prevalent labels\n",
        "y_train_labels = mode(y_train, axis=(1, 2))[0].flatten()\n",
        "\n",
        "print(f\"Shape X_train: {X_train.shape}\")\n",
        "print(f\"Shape y_train_labels: {y_train_labels.shape}\")\n",
        "\n",
        "# List all unique labels to check correctness\n",
        "unique_labels = np.unique(y_train)\n",
        "print(f\"Unique classes: {unique_labels}\")\n",
        "\n",
        "# Plot images in batches\n",
        "def plot_images(X, y, start_index=0, images_per_row=10, images_per_col=10):\n",
        "    fig, axes = plt.subplots(images_per_col, images_per_row, figsize=(15, 15))\n",
        "    for i in range(images_per_row * images_per_col):\n",
        "        idx = start_index + i\n",
        "        if idx >= len(X):\n",
        "            break\n",
        "        ax = axes[i // images_per_row, i % images_per_row]\n",
        "        ax.imshow(X[idx], cmap=\"gray\")\n",
        "        ax.set_title(f\"Class: {y[idx]}\")\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot a sample image from each class\n",
        "def plot_one_sample_per_class(X, y, y_mask, classes):\n",
        "    for label in classes:\n",
        "        for i in range(len(y_mask)):\n",
        "            if label in np.unique(y_mask[i]):\n",
        "                plt.figure()\n",
        "                plt.imshow(X[i], cmap=\"gray\")\n",
        "                plt.title(f\"Class: {label}\")\n",
        "                plt.axis(\"off\")\n",
        "                plt.show()\n",
        "                break\n",
        "\n",
        "plot_one_sample_per_class(X_train, y_train_labels, y_train, unique_labels)\n",
        "\n",
        "# Plot all images\n",
        "images_per_row = 10\n",
        "images_per_col = 10\n",
        "images_per_page = images_per_row * images_per_col\n",
        "num_images = X_train.shape[0]\n",
        "\n",
        "for start_idx in range(0, num_images, images_per_page):\n",
        "    plot_images(X_train, y_train_labels, start_index=start_idx, images_per_row=images_per_row, images_per_col=images_per_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z6VS98FeMBD"
      },
      "source": [
        "## âŒ Remove outliers from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PzVZbyNco6v",
        "outputId": "81bc133c-2e5c-4a6b-a955-2e72edf2887e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape X_train_filtered: (2505, 64, 128, 1)\n",
            "Shape y_train_filtered: (2505, 64, 128)\n",
            "Unique classes: [0. 1. 2. 3. 4.]\n"
          ]
        }
      ],
      "source": [
        "# Lists to contain filtered elements\n",
        "X_train_filtered = []\n",
        "y_train_filtered = []\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    label = y_train[i].argmax() if y_train.ndim > 1 else y_train[i]\n",
        "    if label != 415:\n",
        "        # Add to filtered dataset the non-alien images\n",
        "        X_train_filtered.append(X_train[i])\n",
        "        y_train_filtered.append(y_train[i])\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train_filtered = np.array(X_train_filtered)\n",
        "y_train_filtered = np.array(y_train_filtered)\n",
        "\n",
        "print(f\"Shape X_train_filtered: {X_train_filtered.shape}\")\n",
        "print(f\"Shape y_train_filtered: {y_train_filtered.shape}\")\n",
        "print(f\"Unique classes: {np.unique(y_train_filtered)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fE76Lu-Ea0-"
      },
      "source": [
        "## ğŸ” Inspect the filtered training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ibagIaQkHRiK",
        "outputId": "2b493ef8-697d-416a-b014-2fb599f86476"
      },
      "outputs": [],
      "source": [
        "num_images_filtered = X_train_filtered.shape[0]\n",
        "y_train_filtered_labels = mode(y_train_filtered, axis=(1, 2))[0].flatten()\n",
        "\n",
        "# Plot the filtered dataset\n",
        "for start_idx in range(0, num_images_filtered, images_per_page):\n",
        "    plot_images(X_train_filtered, y_train_filtered_labels, start_index=start_idx, images_per_row=images_per_row, images_per_col=images_per_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsXRC_eIlqdY"
      },
      "source": [
        "## ğŸ§® Define network parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB8MXxkwltdh"
      },
      "outputs": [],
      "source": [
        "# Set batch size for training\n",
        "batch_size = 64\n",
        "\n",
        "# Set learning rate for the optimiser\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Set early stopping patience threshold\n",
        "patience = 10\n",
        "\n",
        "# Set maximum number of training epochs\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEb5t0AgmfQc"
      },
      "source": [
        "## âœ‚ Split into Training and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVf9fnTumtuP",
        "outputId": "01281cfb-443a-4f88-938b-7aa73645685d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape:\t (2004, 64, 128, 1) (2004, 64, 128)\n",
            "Validation set shape:\t (501, 64, 128, 1) (501, 64, 128)\n",
            "Class weights: [  0.8245642   0.5963      0.8436091   1.0896032 141.21344  ]\n"
          ]
        }
      ],
      "source": [
        "# Split the training dataset to get a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_filtered,\n",
        "    y_train_filtered,\n",
        "    test_size=0.2,\n",
        "    random_state=seed)\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.flatten())\n",
        "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
        "class_weights = [class_weights[key] for key in sorted(class_weights.keys())]\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print('Training set shape:\\t', X_train.shape, y_train.shape)\n",
        "print('Validation set shape:\\t', X_val.shape, y_val.shape)\n",
        "\n",
        "# Convert in un tensore statico\n",
        "class_weights = tf.constant(class_weights, dtype=tf.float32)\n",
        "print(f\"Class weights: {class_weights}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVztd7_HgvOq"
      },
      "source": [
        "## ğŸ”„ Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_laNhjCPgxQo"
      },
      "outputs": [],
      "source": [
        "def augment_data(image, label):\n",
        "    # Geometric Transformations\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    label = tf.image.random_flip_left_right(label)\n",
        "\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    label = tf.image.random_flip_up_down(label)\n",
        "\n",
        "    # Chromatic Transformations\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = tf.expand_dims(image, axis=-1) if len(image.shape) == 2 else image\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    return image\n",
        "\n",
        "def preprocess_label(label):\n",
        "    label = tf.expand_dims(label, axis=-1) if len(label.shape) == 2 else label\n",
        "    label = tf.cast(label, tf.int32)\n",
        "    return label\n",
        "\n",
        "def preprocess_data(image, label):\n",
        "    image = preprocess_image(image)\n",
        "    label = preprocess_label(label)\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnFLkc1chG53",
        "outputId": "e4548b81-6cf1-4fb8-86e2-afd338dc9b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Dataset pixel distribution:\t {0: 7963852, 1: 11012422, 2: 7784064, 3: 6026696, 4: 46502}\n"
          ]
        }
      ],
      "source": [
        "# Original dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Augmented dataset\n",
        "augmented_dataset = train_dataset.map(lambda x, y: augment_data(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Combined dataset, having both augmented and original dataset\n",
        "combined_dataset = train_dataset.concatenate(augmented_dataset)\n",
        "combined_dataset = combined_dataset.shuffle(buffer_size=len(X_train))\n",
        "combined_dataset = combined_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "def pixel_distribution(dataset, num_classes):\n",
        "    class_counts = {i: 0 for i in range(num_classes)}\n",
        "    for _, label in dataset.unbatch():\n",
        "        unique, counts = np.unique(label.numpy(), return_counts=True)\n",
        "        for u, c in zip(unique, counts):\n",
        "            class_counts[int(u)] += c\n",
        "    return class_counts\n",
        "\n",
        "print(\"Combined Dataset pixel distribution:\\t\", pixel_distribution(combined_dataset, num_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUpegWw8SLNr"
      },
      "source": [
        "## ğŸ”¨ Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH96h-n22oOo"
      },
      "outputs": [],
      "source": [
        "def se_block(input_tensor, reduction_ratio=16):\n",
        "    filters = input_tensor.shape[-1]\n",
        "    se = tfkl.GlobalAveragePooling2D()(input_tensor)\n",
        "    se = tfkl.Dense(filters // reduction_ratio, activation='relu', kernel_initializer='he_normal')(se)\n",
        "    se = tfkl.Dense(filters, activation='sigmoid', kernel_initializer='he_normal')(se)\n",
        "    se = tfkl.Reshape((1, 1, filters))(se)\n",
        "    return tfkl.Multiply()([input_tensor, se])\n",
        "\n",
        "class AdaptShapeLayer(tfkl.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AdaptShapeLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, tensor, target_tensor):\n",
        "        target_shape = tf.shape(target_tensor)[1:3]\n",
        "        tensor = tf.image.resize(tensor, target_shape, method='bilinear')\n",
        "        return tensor\n",
        "\n",
        "def adapt_channels(tensor, target_channels):\n",
        "    conv = tfkl.Conv2D(target_channels, kernel_size=1, padding='same')\n",
        "    return conv(tensor)\n",
        "\n",
        "def bottleneck_layer(input_tensor, filters, reduction_ratio=4):\n",
        "    adapt_shape = AdaptShapeLayer()\n",
        "    residual = adapt_channels(input_tensor, filters)\n",
        "\n",
        "    # Bottleneck structure\n",
        "    reduced_filters = filters // reduction_ratio\n",
        "    x = tfkl.Conv2D(reduced_filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(input_tensor)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.SeparableConv2D(filters, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "    x = se_block(x)\n",
        "    x = tfkl.Dropout(0.3)(x)\n",
        "    residual = adapt_shape(residual, x)\n",
        "    x = tfkl.Add()([x, residual])\n",
        "    return x\n",
        "\n",
        "def unet_block(input_tensor, filters, kernel_size=3, stack=4, name=''):\n",
        "    x = input_tensor\n",
        "    adapt_shape = AdaptShapeLayer()\n",
        "\n",
        "    # Save for residual connection\n",
        "    residual = adapt_channels(input_tensor, filters)\n",
        "\n",
        "    for i in range(stack):\n",
        "        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = tfkl.BatchNormalization()(x)\n",
        "        x = se_block(x)\n",
        "        x = tfkl.Activation(tf.nn.leaky_relu)(x)\n",
        "        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = tfkl.BatchNormalization()(x)\n",
        "        x = se_block(x)\n",
        "        x = tfkl.Activation(tf.nn.leaky_relu)(x)\n",
        "        x = tfkl.SpatialDropout2D(0.2)(x)\n",
        "\n",
        "    # Adapt shapes for residual connection\n",
        "    residual = adapt_shape(residual, x)\n",
        "    x = tfkl.Add()([x, residual])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zptfRq7QMkD"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes, axis=-1)\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    dice = (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
        "    return 1 - dice\n",
        "\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    gamma = tf.constant(gamma, dtype=tf.float32)\n",
        "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=y_pred.shape[-1])\n",
        "        if len(y_true_one_hot.shape) > len(y_pred.shape):\n",
        "            y_true_one_hot = tf.squeeze(y_true_one_hot, axis=-2)\n",
        "        cross_entropy = -y_true_one_hot * tf.keras.backend.log(y_pred)\n",
        "        weight = alpha * tf.math.pow((1 - y_pred), gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "def combined_loss(y_true, y_pred, dice_weight=0.5, focal_weight=0.5):\n",
        "    dice = dice_loss(y_true, y_pred, smooth=1e-6)\n",
        "    focal = focal_loss(gamma=2.0, alpha=0.25)(y_true, y_pred)\n",
        "    return dice_weight * dice + focal_weight * focal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CBkb3TRF1KJx"
      },
      "outputs": [],
      "source": [
        "inputs = tfkl.Input(shape=input_shape)\n",
        "\n",
        "# Downsampling\n",
        "down_block_1 = unet_block(inputs, 64)\n",
        "d1 = tfkl.MaxPooling2D()(down_block_1)\n",
        "\n",
        "down_block_2 = unet_block(d1, 128)\n",
        "d2 = tfkl.MaxPooling2D()(down_block_2)\n",
        "\n",
        "down_block_3 = unet_block(d2, 256)\n",
        "d3 = tfkl.MaxPooling2D()(down_block_3)\n",
        "\n",
        "down_block_4 = unet_block(d3, 512)\n",
        "d4 = tfkl.MaxPooling2D()(down_block_4)\n",
        "\n",
        "# Bottleneck\n",
        "bottleneck = bottleneck_layer(d4, 1024)\n",
        "\n",
        "# Upsampling\n",
        "u1 = tfkl.Conv2DTranspose(512, kernel_size=2, strides=2, padding='same')(bottleneck)\n",
        "u1 = tfkl.Concatenate()([u1, down_block_4])\n",
        "u1 = unet_block(u1, 512)\n",
        "\n",
        "u2 = tfkl.Conv2DTranspose(256, kernel_size=2, strides=2, padding='same')(u1)\n",
        "u2 = tfkl.Concatenate()([u2, down_block_3])\n",
        "u2 = unet_block(u2, 256)\n",
        "\n",
        "u3 = tfkl.Conv2DTranspose(128, kernel_size=2, strides=2, padding='same')(u2)\n",
        "u3 = tfkl.Concatenate()([u3, down_block_2])\n",
        "u3 = unet_block(u3, 128)\n",
        "\n",
        "u4 = tfkl.Conv2DTranspose(64, kernel_size=2, strides=2, padding='same')(u3)\n",
        "u4 = tfkl.Concatenate()([u4, down_block_1])\n",
        "u4 = unet_block(u4, 64)\n",
        "\n",
        "# Output\n",
        "outputs = tfkl.Conv2D(num_classes, kernel_size=1, activation='softmax')(u4)\n",
        "\n",
        "model = tfk.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Define the MeanIoU ignoring the background class\n",
        "mean_iou = tfk.metrics.MeanIoU(num_classes=num_classes, ignore_class=0, sparse_y_pred=False, name='mean_iou')\n",
        "optimizer = tfk.optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-5)\n",
        "loss = combined_loss\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[mean_iou])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSliIxBvbs2Q"
      },
      "source": [
        "## ğŸ› ï¸ Train and Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7naWi9Y8vjD"
      },
      "outputs": [],
      "source": [
        "# Create an EarlyStopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Create a LearningRate Scheduler\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5\n",
        ")\n",
        "\n",
        "# Store the callbacks in a list\n",
        "callbacks = [early_stopping, lr_scheduler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pMCbSMQ_-XoH",
        "outputId": "2426e623-de85-4ecd-a2ce-f63790c3c6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 3s/step - loss: 0.5272 - mean_iou: 0.1076 - val_loss: 0.4752 - val_mean_iou: 0.0937 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.4494 - mean_iou: 0.1567 - val_loss: 0.5381 - val_mean_iou: 0.1319 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 218ms/step - loss: 0.4226 - mean_iou: 0.2026 - val_loss: 0.4353 - val_mean_iou: 0.1420 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.4009 - mean_iou: 0.2259 - val_loss: 0.4993 - val_mean_iou: 0.1169 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.3865 - mean_iou: 0.2354 - val_loss: 0.5124 - val_mean_iou: 0.1059 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.3733 - mean_iou: 0.2587 - val_loss: 0.4795 - val_mean_iou: 0.1350 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.3715 - mean_iou: 0.2649 - val_loss: 0.4293 - val_mean_iou: 0.1634 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step - loss: 0.3531 - mean_iou: 0.2915 - val_loss: 0.3944 - val_mean_iou: 0.2182 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.3608 - mean_iou: 0.2846 - val_loss: 0.4578 - val_mean_iou: 0.1731 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.3976 - mean_iou: 0.2409 - val_loss: 0.3930 - val_mean_iou: 0.2310 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.3415 - mean_iou: 0.3073 - val_loss: 0.3235 - val_mean_iou: 0.3202 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step - loss: 0.3327 - mean_iou: 0.3121 - val_loss: 0.3091 - val_mean_iou: 0.3269 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.3168 - mean_iou: 0.3309 - val_loss: 0.3408 - val_mean_iou: 0.2911 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.3236 - mean_iou: 0.3173 - val_loss: 0.3242 - val_mean_iou: 0.2809 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.3139 - mean_iou: 0.3418 - val_loss: 0.3376 - val_mean_iou: 0.2902 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step - loss: 0.3051 - mean_iou: 0.3424 - val_loss: 0.2926 - val_mean_iou: 0.3706 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.3057 - mean_iou: 0.3421 - val_loss: 0.3606 - val_mean_iou: 0.2791 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step - loss: 0.2981 - mean_iou: 0.3616 - val_loss: 0.2870 - val_mean_iou: 0.3499 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.3000 - mean_iou: 0.3524 - val_loss: 0.3552 - val_mean_iou: 0.2721 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.2953 - mean_iou: 0.3636 - val_loss: 0.2833 - val_mean_iou: 0.3736 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2900 - mean_iou: 0.3578 - val_loss: 0.3404 - val_mean_iou: 0.2740 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.2845 - mean_iou: 0.3765 - val_loss: 0.3650 - val_mean_iou: 0.2420 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2875 - mean_iou: 0.3690 - val_loss: 0.2991 - val_mean_iou: 0.3571 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 218ms/step - loss: 0.2866 - mean_iou: 0.3704 - val_loss: 0.2631 - val_mean_iou: 0.3868 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2819 - mean_iou: 0.3737 - val_loss: 0.3152 - val_mean_iou: 0.3322 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 218ms/step - loss: 0.2822 - mean_iou: 0.3752 - val_loss: 0.2598 - val_mean_iou: 0.3947 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2745 - mean_iou: 0.3799 - val_loss: 0.3698 - val_mean_iou: 0.2758 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.2734 - mean_iou: 0.3892 - val_loss: 0.2499 - val_mean_iou: 0.4227 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.2742 - mean_iou: 0.3834 - val_loss: 0.2452 - val_mean_iou: 0.4168 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2761 - mean_iou: 0.3878 - val_loss: 0.2755 - val_mean_iou: 0.4049 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2714 - mean_iou: 0.3965 - val_loss: 0.2991 - val_mean_iou: 0.3546 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 215ms/step - loss: 0.2690 - mean_iou: 0.3945 - val_loss: 0.2922 - val_mean_iou: 0.3175 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.2783 - mean_iou: 0.3768 - val_loss: 0.2766 - val_mean_iou: 0.3302 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2627 - mean_iou: 0.4072 - val_loss: 0.3851 - val_mean_iou: 0.2941 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.2647 - mean_iou: 0.4081 - val_loss: 0.3018 - val_mean_iou: 0.3617 - learning_rate: 5.0000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2472 - mean_iou: 0.4203 - val_loss: 0.2507 - val_mean_iou: 0.4091 - learning_rate: 5.0000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2522 - mean_iou: 0.4156 - val_loss: 0.3129 - val_mean_iou: 0.3543 - learning_rate: 5.0000e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2543 - mean_iou: 0.4166 - val_loss: 0.2699 - val_mean_iou: 0.3993 - learning_rate: 5.0000e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step - loss: 0.2544 - mean_iou: 0.4129 - val_loss: 0.2391 - val_mean_iou: 0.4287 - learning_rate: 5.0000e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step - loss: 0.2458 - mean_iou: 0.4197 - val_loss: 0.2331 - val_mean_iou: 0.4394 - learning_rate: 5.0000e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2431 - mean_iou: 0.4216 - val_loss: 0.2453 - val_mean_iou: 0.4184 - learning_rate: 5.0000e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2453 - mean_iou: 0.4189 - val_loss: 0.2465 - val_mean_iou: 0.4238 - learning_rate: 5.0000e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step - loss: 0.2458 - mean_iou: 0.4306 - val_loss: 0.2321 - val_mean_iou: 0.4399 - learning_rate: 5.0000e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2502 - mean_iou: 0.4223 - val_loss: 0.2475 - val_mean_iou: 0.4266 - learning_rate: 5.0000e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2459 - mean_iou: 0.4239 - val_loss: 0.2706 - val_mean_iou: 0.3777 - learning_rate: 5.0000e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.2425 - mean_iou: 0.4288 - val_loss: 0.3039 - val_mean_iou: 0.3384 - learning_rate: 5.0000e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2395 - mean_iou: 0.4300 - val_loss: 0.2432 - val_mean_iou: 0.4259 - learning_rate: 5.0000e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.2433 - mean_iou: 0.4312 - val_loss: 0.2322 - val_mean_iou: 0.4487 - learning_rate: 5.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.2368 - mean_iou: 0.4385 - val_loss: 0.2628 - val_mean_iou: 0.3987 - learning_rate: 2.5000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 211ms/step - loss: 0.2272 - mean_iou: 0.4416 - val_loss: 0.2347 - val_mean_iou: 0.4217 - learning_rate: 2.5000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 215ms/step - loss: 0.2277 - mean_iou: 0.4472 - val_loss: 0.2268 - val_mean_iou: 0.4508 - learning_rate: 2.5000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.2235 - mean_iou: 0.4498 - val_loss: 0.2417 - val_mean_iou: 0.4330 - learning_rate: 2.5000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.2283 - mean_iou: 0.4436 - val_loss: 0.2237 - val_mean_iou: 0.4467 - learning_rate: 2.5000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.2184 - mean_iou: 0.4586 - val_loss: 0.2194 - val_mean_iou: 0.4530 - learning_rate: 2.5000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2291 - mean_iou: 0.4437 - val_loss: 0.2289 - val_mean_iou: 0.4492 - learning_rate: 2.5000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2263 - mean_iou: 0.4427 - val_loss: 0.2369 - val_mean_iou: 0.4359 - learning_rate: 2.5000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2246 - mean_iou: 0.4501 - val_loss: 0.2254 - val_mean_iou: 0.4514 - learning_rate: 2.5000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2216 - mean_iou: 0.4568 - val_loss: 0.2372 - val_mean_iou: 0.4397 - learning_rate: 2.5000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2201 - mean_iou: 0.4579 - val_loss: 0.2456 - val_mean_iou: 0.4223 - learning_rate: 2.5000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2205 - mean_iou: 0.4596 - val_loss: 0.2230 - val_mean_iou: 0.4494 - learning_rate: 1.2500e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.2184 - mean_iou: 0.4550 - val_loss: 0.2193 - val_mean_iou: 0.4550 - learning_rate: 1.2500e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2141 - mean_iou: 0.4609 - val_loss: 0.2245 - val_mean_iou: 0.4470 - learning_rate: 1.2500e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2068 - mean_iou: 0.4733 - val_loss: 0.2263 - val_mean_iou: 0.4462 - learning_rate: 1.2500e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2123 - mean_iou: 0.4620 - val_loss: 0.2254 - val_mean_iou: 0.4363 - learning_rate: 1.2500e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - loss: 0.2130 - mean_iou: 0.4641 - val_loss: 0.2185 - val_mean_iou: 0.4594 - learning_rate: 6.2500e-05\n",
            "Epoch 66/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2013 - mean_iou: 0.4732 - val_loss: 0.2196 - val_mean_iou: 0.4494 - learning_rate: 6.2500e-05\n",
            "Epoch 67/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2075 - mean_iou: 0.4678 - val_loss: 0.2189 - val_mean_iou: 0.4480 - learning_rate: 6.2500e-05\n",
            "Epoch 68/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2055 - mean_iou: 0.4721 - val_loss: 0.2223 - val_mean_iou: 0.4446 - learning_rate: 6.2500e-05\n",
            "Epoch 69/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step - loss: 0.2037 - mean_iou: 0.4753 - val_loss: 0.2150 - val_mean_iou: 0.4646 - learning_rate: 6.2500e-05\n",
            "Epoch 70/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.2015 - mean_iou: 0.4774 - val_loss: 0.2168 - val_mean_iou: 0.4562 - learning_rate: 6.2500e-05\n",
            "Epoch 71/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2039 - mean_iou: 0.4744 - val_loss: 0.2220 - val_mean_iou: 0.4522 - learning_rate: 6.2500e-05\n",
            "Epoch 72/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 214ms/step - loss: 0.2004 - mean_iou: 0.4715 - val_loss: 0.2197 - val_mean_iou: 0.4549 - learning_rate: 6.2500e-05\n",
            "Epoch 73/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.1998 - mean_iou: 0.4735 - val_loss: 0.2156 - val_mean_iou: 0.4613 - learning_rate: 6.2500e-05\n",
            "Epoch 74/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2012 - mean_iou: 0.4751 - val_loss: 0.2178 - val_mean_iou: 0.4607 - learning_rate: 6.2500e-05\n",
            "Epoch 75/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 213ms/step - loss: 0.1975 - mean_iou: 0.4778 - val_loss: 0.2182 - val_mean_iou: 0.4543 - learning_rate: 3.1250e-05\n",
            "Epoch 76/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.2015 - mean_iou: 0.4739 - val_loss: 0.2171 - val_mean_iou: 0.4599 - learning_rate: 3.1250e-05\n",
            "Epoch 77/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.1977 - mean_iou: 0.4767 - val_loss: 0.2175 - val_mean_iou: 0.4570 - learning_rate: 3.1250e-05\n",
            "Epoch 78/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.1981 - mean_iou: 0.4772 - val_loss: 0.2150 - val_mean_iou: 0.4605 - learning_rate: 3.1250e-05\n",
            "Epoch 79/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 212ms/step - loss: 0.1969 - mean_iou: 0.4814 - val_loss: 0.2188 - val_mean_iou: 0.4522 - learning_rate: 3.1250e-05\n",
            "Final validation Mean Intersection Over Union: 46.46%\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    combined_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=callbacks\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_meanIoU = round(max(history['val_mean_iou'])* 100, 2)\n",
        "print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtM0ubgdOzG-"
      },
      "outputs": [],
      "source": [
        "timestep_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "model_filename = f\"model_{timestep_str}.keras\"\n",
        "model.save(model_filename)\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNp6pUZuddqC"
      },
      "source": [
        "## ğŸ“Š Test the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU00iEFcYi_X",
        "outputId": "3cb92865-d4df-4d5a-d483-6b783ff95504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m314/314\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 55ms/step\n",
            "Predictions shape: (10022, 64, 128)\n"
          ]
        }
      ],
      "source": [
        "model = tfk.models.load_model(model_filename, custom_objects={\n",
        "        'dice_loss': dice_loss,\n",
        "        'focal_loss': focal_loss,\n",
        "        'combined_loss': combined_loss,\n",
        "        'unet_block': unet_block,\n",
        "        'bottleneck_layer': bottleneck_layer,\n",
        "        'se_block': se_block,\n",
        "        'AdaptShapeLayer': AdaptShapeLayer,\n",
        "        'adapt_channels': adapt_channels\n",
        "    }\n",
        ")\n",
        "\n",
        "preds = model.predict(X_test)\n",
        "preds = np.argmax(preds, axis=-1)\n",
        "print(f\"Predictions shape: {preds.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KErlLGwOTsX6"
      },
      "source": [
        "## ğŸ’¾ Save the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPjMEKqZW5jX"
      },
      "outputs": [],
      "source": [
        "def y_to_df(y) -> pd.DataFrame:\n",
        "    n_samples = len(y)\n",
        "    y_flat = y.reshape(n_samples, -1)\n",
        "    df = pd.DataFrame(y_flat)\n",
        "    df[\"id\"] = np.arange(n_samples)\n",
        "    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
        "    return df[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s18kX1uDconq"
      },
      "outputs": [],
      "source": [
        "# Create the csv submission file\n",
        "timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n",
        "submission_filename = f\"submission_{timestep_str}.csv\"\n",
        "submission_df = y_to_df(preds)\n",
        "submission_df.to_csv(submission_filename, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dw_-hFm6bjY6",
        "d7IqZP5Iblna",
        "GN_cpHlSboXV",
        "A3VPMx3wpqVd",
        "2z6VS98FeMBD",
        "9fE76Lu-Ea0-",
        "dsXRC_eIlqdY",
        "KEb5t0AgmfQc",
        "OVztd7_HgvOq",
        "vUpegWw8SLNr",
        "RNp6pUZuddqC",
        "KErlLGwOTsX6"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
