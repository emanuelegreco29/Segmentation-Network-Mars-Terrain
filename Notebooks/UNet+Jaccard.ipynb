{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw_-hFm6bjY6"
   },
   "source": [
    "## ðŸŒ Connect Colab to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2S4GWr3Uoa8",
    "outputId": "bf58ef13-7214-42ee-effd-445a2b536246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive/My Drive\n",
      "/gdrive/My Drive/[2024-2025] AN2DL/Homework 2\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/gdrive\")\n",
    "%cd /gdrive/My Drive\n",
    "%cd [2024-2025] AN2DL/Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## âš™ï¸ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO6_Ft_8T56A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.stats import mode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Set seed for TensorFlow\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# Reduce TensorFlow verbosity\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## â³ Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLaoDaG1V1Yg",
    "outputId": "6b280182-330f-4a7d-a346-a0078ab5dbf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X shape: (2615, 64, 128)\n",
      "Training y shape: (2615, 64, 128)\n",
      "Test X shape: (10022, 64, 128)\n",
      "Input shape: (64, 128, 1)\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"mars_for_students.npz\")\n",
    "\n",
    "training_set = data[\"training_set\"]\n",
    "X_train = training_set[:, 0]\n",
    "y_train = training_set[:, 1]\n",
    "\n",
    "X_test = data[\"test_set\"]\n",
    "\n",
    "print(f\"Training X shape: {X_train.shape}\")\n",
    "print(f\"Training y shape: {y_train.shape}\")\n",
    "print(f\"Test X shape: {X_test.shape}\")\n",
    "\n",
    "# Add color channel and rescale pixels between 0 and 1\n",
    "X_train = X_train[..., np.newaxis] / 255.0\n",
    "X_test = X_test[..., np.newaxis] / 255.0\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3VPMx3wpqVd"
   },
   "source": [
    "## ðŸ” Inspect the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "Y5_2vtLTpufm",
    "outputId": "27145ca1-1db6-4561-daf2-07f6a80adee5"
   },
   "outputs": [],
   "source": [
    "# Extract all the dominant labels\n",
    "y_train_labels = mode(y_train, axis=(1, 2))[0].flatten()\n",
    "unique_labels = np.unique(y_train)\n",
    "\n",
    "# Plot images in batches\n",
    "def plot_images(X, y, start_index=0, img_row=10, img_col=10):\n",
    "    fig, axes = plt.subplots(img_col, img_row, figsize=(15, 15))\n",
    "    for i in range(img_row * img_col):\n",
    "        idx = start_index + i\n",
    "        if idx >= len(X):\n",
    "            break\n",
    "        ax = axes[i // img_row, i % img_row]\n",
    "        ax.imshow(X[idx], cmap=\"gray\")\n",
    "        ax.set_title(f\"Class: {y[idx]}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot just one image from each class\n",
    "def plot_one(X, y, y_mask, classes):\n",
    "    for label in classes:\n",
    "        for i in range(len(y_mask)):\n",
    "            if label in np.unique(y_mask[i]):\n",
    "                plt.figure()\n",
    "                plt.imshow(X[i], cmap=\"gray\")\n",
    "                plt.title(f\"Class: {label}\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "                break\n",
    "\n",
    "plot_one(X_train, y_train_labels, y_train, unique_labels)\n",
    "\n",
    "# Plot all the images in batches\n",
    "img_row = 10\n",
    "img_col = 10\n",
    "img_page = img_row * img_col\n",
    "num_images = X_train.shape[0]\n",
    "\n",
    "for start_idx in range(0, num_images, img_page):\n",
    "    plot_images(X_train, y_train_labels, start_index=start_idx, img_row=img_row, img_col=img_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VF__w-rtC9em"
   },
   "source": [
    "## âŒ Remove outliers from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uuo2Oa0KC-R6",
    "outputId": "d7e69337-3f76-4a6e-db15-d96bf8b305ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train_filtered: (2505, 64, 128, 1)\n",
      "Shape y_train_filtered: (2505, 64, 128)\n",
      "Unique classes: [0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Lists to contain filtered elements\n",
    "X_train_filtered = []\n",
    "y_train_filtered = []\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    label = y_train[i].argmax() if y_train.ndim > 1 else y_train[i]\n",
    "    if label != 415:\n",
    "        # Add to filtered dataset the non-alien images\n",
    "        X_train_filtered.append(X_train[i])\n",
    "        y_train_filtered.append(y_train[i])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_filtered = np.array(X_train_filtered)\n",
    "y_train_filtered = np.array(y_train_filtered)\n",
    "\n",
    "print(f\"Shape X_train_filtered: {X_train_filtered.shape}\")\n",
    "print(f\"Shape y_train_filtered: {y_train_filtered.shape}\")\n",
    "print(f\"Unique classes: {np.unique(y_train_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEb5t0AgmfQc"
   },
   "source": [
    "## âœ‚ Split into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVf9fnTumtuP",
    "outputId": "277f5335-fc60-4262-f5dd-251eab3bc5b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:\t (2353, 64, 128, 1) (2353, 64, 128)\n",
      "Validation set shape:\t (262, 64, 128, 1) (262, 64, 128)\n"
     ]
    }
   ],
   "source": [
    "# Split the training dataset to get a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.1,\n",
    "    random_state=seed)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print('Training set shape:\\t', X_train.shape, y_train.shape)\n",
    "print('Validation set shape:\\t', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsXRC_eIlqdY"
   },
   "source": [
    "## ðŸ§® Define network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QB8MXxkwltdh",
    "outputId": "fdf733e2-70f1-4516-c4ff-b529d20686da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.0, 1: 0.6075654375216993, 2: 0.8680682195342838, 3: 1.0852267596292755, 4: 151.75983938904855}\n"
     ]
    }
   ],
   "source": [
    "# Set batch size for training\n",
    "batch_size = 64\n",
    "\n",
    "# Set learning rate for the optimizer\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Set early stopping patience threshold\n",
    "patience = 10\n",
    "\n",
    "# Set maximum number of training epochs\n",
    "epochs = 100\n",
    "\n",
    "# Flat y_train to compute weights\n",
    "y_train_flat = y_train.flatten()\n",
    "\n",
    "# Compute class weights\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_flat),\n",
    "    y=y_train_flat\n",
    ")\n",
    "\n",
    "# Convert weights to dictionary, set background to 0\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "class_weights[0] = 0.0\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "te5Ey7pNoeiw"
   },
   "outputs": [],
   "source": [
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_mean_iou',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_mean_iou\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Store the callback in a list\n",
    "callbacks = [early_stopping, lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GfaATYL9SXj"
   },
   "source": [
    "## ðŸ”„ Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mZ7Ufm---Vpq"
   },
   "outputs": [],
   "source": [
    "def augment_data(image, label):\n",
    "    # Geometric Transformations\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    label = tf.image.random_flip_left_right(label)\n",
    "\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    label = tf.image.random_flip_up_down(label)\n",
    "\n",
    "    # Chromatic Transformations\n",
    "    image = tf.image.random_brightness(image, max_delta=0.4)\n",
    "    image = tf.image.random_contrast(image, lower=0.7, upper=1.3)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.expand_dims(image, axis=-1) if len(image.shape) == 2 else image\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image\n",
    "\n",
    "def preprocess_label(label):\n",
    "    label = tf.expand_dims(label, axis=-1) if len(label.shape) == 2 else label\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    return label\n",
    "\n",
    "def preprocess_data(image, label):\n",
    "    image = preprocess_image(image)\n",
    "    label = preprocess_label(label)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIDvSbjs33zy"
   },
   "outputs": [],
   "source": [
    "# Original dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Augmented dataset\n",
    "augmented_dataset = train_dataset.map(lambda x, y: augment_data(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Combined dataset, having both augmented and original dataset\n",
    "combined_dataset = train_dataset.concatenate(augmented_dataset)\n",
    "combined_dataset = combined_dataset.shuffle(buffer_size=len(X_train))\n",
    "combined_dataset = combined_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUpegWw8SLNr"
   },
   "source": [
    "## ðŸ”¨ Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lD54qYQNdDL"
   },
   "outputs": [],
   "source": [
    "def res_unet_block(input_tensor, filters, name):\n",
    "    x = tfkl.Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", name=f\"{name}_conv1\")(input_tensor)\n",
    "    x = tfkl.BatchNormalization(name=f\"{name}_bn1\")(x)\n",
    "    x = tfkl.Activation(\"relu\", name=f\"{name}_act1\")(x)\n",
    "    x = tfkl.Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", name=f\"{name}_conv2\")(x)\n",
    "    x = tfkl.BatchNormalization(name=f\"{name}_bn2\")(x)\n",
    "\n",
    "    # Skip connection\n",
    "    shortcut = tfkl.Conv2D(filters, (1, 1), padding=\"same\", name=f\"{name}_shortcut\")(input_tensor)\n",
    "    x = tfkl.Add(name=f\"{name}_add\")([x, shortcut])\n",
    "    x = tfkl.Activation(\"relu\", name=f\"{name}_act2\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CBkb3TRF1KJx",
    "outputId": "2682575b-ff72-43b7-eb00-53865b76bf4d"
   },
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "input_layer = tfkl.Input(shape=input_shape)\n",
    "\n",
    "# Downsampling path with an extra layer\n",
    "def downsampling_block(input_tensor, filters,):\n",
    "    x = res_unet_block(input_tensor, filters)\n",
    "    # Extra convolutional layer\n",
    "    x = tfkl.Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    pool = tfkl.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    return x, pool\n",
    "\n",
    "down_block_1, d1 = downsampling_block(input_layer, 64)\n",
    "down_block_2, d2 = downsampling_block(d1, 128)\n",
    "down_block_3, d3 = downsampling_block(d2, 256)\n",
    "down_block_4, d4 = downsampling_block(d3, 512)\n",
    "\n",
    "# Bottleneck with multi-scale features and Squeeze-and-Excitation\n",
    "def bottleneck_module(input_tensor, filters):\n",
    "    # Base convolution\n",
    "    x = tfkl.Conv2D(filters, kernel_size=3, padding='same', activation='relu')(input_tensor)\n",
    "\n",
    "    # Squeeze-and-Excitation Block\n",
    "    se = tfkl.GlobalAveragePooling2D()(x)\n",
    "    se = tfkl.Dense(filters // 16, activation='relu')(se)\n",
    "    se = tfkl.Dense(filters, activation='sigmoid')(se)\n",
    "    se = tfkl.Reshape((1, 1, filters))(se)\n",
    "\n",
    "    # Amplification of important features\n",
    "    return tfkl.Multiply()([x, se])\n",
    "\n",
    "bottleneck = bottleneck_module(d4, 1024)\n",
    "bottleneck = tfkl.SpatialDropout2D(0.4)(bottleneck)\n",
    "\n",
    "# Upsampling path with skip connections\n",
    "def upsampling_block(input_tensor, skip_connection, filters):\n",
    "    up = tfkl.Conv2DTranspose(filters, kernel_size=2, strides=2, padding=\"same\")(input_tensor)\n",
    "    skip_connection = tfkl.Dropout(0.2)(skip_connection)  # Dropout sulle connessioni skip\n",
    "    concat = tfkl.Concatenate()([up, skip_connection])\n",
    "    x = res_unet_block(concat, filters)\n",
    "    # Extra convolutional layer\n",
    "    x = tfkl.Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Activation(\"relu\")(x)\n",
    "    return tfkl.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "# Upsampling path\n",
    "u4 = upsampling_block(bottleneck, down_block_4, 512)\n",
    "u3 = upsampling_block(u4, down_block_3, 256)\n",
    "u2 = upsampling_block(u3, down_block_2, 128)\n",
    "u1 = upsampling_block(u2, down_block_1, 64)\n",
    "\n",
    "# Output layer with softmax activation for multi-class segmentation\n",
    "output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\")(u1)\n",
    "\n",
    "# Define the model\n",
    "model = tfk.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "def jaccard_loss(y_true, y_pred, smooth=1e-6):\n",
    "    # Convert to one-hot encoding\n",
    "    y_true = tf.squeeze(y_true, axis=-1)\n",
    "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=y_pred.shape[-1])\n",
    "\n",
    "    # Ignore class 0 (background)\n",
    "    y_true = y_true[..., 1:]\n",
    "    y_pred = y_pred[..., 1:]\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[1, 2]) - intersection\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "\n",
    "    # Jaccard Loss\n",
    "    return 1 - tf.reduce_mean(iou)\n",
    "\n",
    "\n",
    "# Define the MeanIoU ignoring the background class\n",
    "mean_iou = tfk.metrics.MeanIoU(num_classes=num_classes, ignore_class=0, sparse_y_pred=False, name='mean_iou')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5),\n",
    "    loss=jaccard_loss,\n",
    "    metrics=[mean_iou]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSliIxBvbs2Q"
   },
   "source": [
    "## ðŸ› ï¸ Train and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMCbSMQ_-XoH",
    "outputId": "95ed68fd-2763-46c4-d409-1f470c84f167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 887ms/step - loss: 0.8993 - mean_iou: 0.1977 - val_loss: 0.9200 - val_mean_iou: 0.1331 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 590ms/step - loss: 0.8442 - mean_iou: 0.3347 - val_loss: 0.9163 - val_mean_iou: 0.1466 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 599ms/step - loss: 0.8347 - mean_iou: 0.4452 - val_loss: 0.9314 - val_mean_iou: 0.1196 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 608ms/step - loss: 0.8298 - mean_iou: 0.4551 - val_loss: 0.9144 - val_mean_iou: 0.1739 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 616ms/step - loss: 0.8298 - mean_iou: 0.4701 - val_loss: 0.8215 - val_mean_iou: 0.4799 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.8282 - mean_iou: 0.4770 - val_loss: 0.8228 - val_mean_iou: 0.4699 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 619ms/step - loss: 0.8221 - mean_iou: 0.5079 - val_loss: 0.8209 - val_mean_iou: 0.4913 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 616ms/step - loss: 0.8210 - mean_iou: 0.4735 - val_loss: 0.8033 - val_mean_iou: 0.5629 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 616ms/step - loss: 0.8162 - mean_iou: 0.4581 - val_loss: 0.8101 - val_mean_iou: 0.5300 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 618ms/step - loss: 0.8206 - mean_iou: 0.4630 - val_loss: 0.8002 - val_mean_iou: 0.5738 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 618ms/step - loss: 0.8128 - mean_iou: 0.5083 - val_loss: 0.7983 - val_mean_iou: 0.5819 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 618ms/step - loss: 0.8121 - mean_iou: 0.5498 - val_loss: 0.7953 - val_mean_iou: 0.5885 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 618ms/step - loss: 0.8149 - mean_iou: 0.4823 - val_loss: 0.7950 - val_mean_iou: 0.5907 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.8069 - mean_iou: 0.5604 - val_loss: 0.7975 - val_mean_iou: 0.5822 - learning_rate: 2.5000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 618ms/step - loss: 0.8033 - mean_iou: 0.5580 - val_loss: 0.7939 - val_mean_iou: 0.6045 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.8041 - mean_iou: 0.5145 - val_loss: 0.7994 - val_mean_iou: 0.5837 - learning_rate: 2.5000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.8079 - mean_iou: 0.5646 - val_loss: 0.7924 - val_mean_iou: 0.6042 - learning_rate: 2.5000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.8056 - mean_iou: 0.4870 - val_loss: 0.7755 - val_mean_iou: 0.5964 - learning_rate: 2.5000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.7740 - mean_iou: 0.4479 - val_loss: 0.7589 - val_mean_iou: 0.5789 - learning_rate: 1.2500e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.7577 - mean_iou: 0.4576 - val_loss: 0.7405 - val_mean_iou: 0.5929 - learning_rate: 1.2500e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.7733 - mean_iou: 0.5215 - val_loss: 0.7528 - val_mean_iou: 0.5579 - learning_rate: 1.2500e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.7489 - mean_iou: 0.5428 - val_loss: 0.7533 - val_mean_iou: 0.5636 - learning_rate: 1.2500e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.7476 - mean_iou: 0.4694 - val_loss: 0.8163 - val_mean_iou: 0.2954 - learning_rate: 1.2500e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.7130 - mean_iou: 0.3498 - val_loss: 0.7437 - val_mean_iou: 0.4340 - learning_rate: 6.2500e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 0.6931 - mean_iou: 0.3433 - val_loss: 0.6963 - val_mean_iou: 0.4628 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    combined_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtM0ubgdOzG-",
    "outputId": "d8ef9951-f60a-4bbd-b630-a734948b8b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model_241211_000402.keras\n"
     ]
    }
   ],
   "source": [
    "timestep_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "model_filename = f\"model_{timestep_str}.keras\"\n",
    "model.save(model_filename)\n",
    "del model\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNp6pUZuddqC"
   },
   "source": [
    "## ðŸ“Š Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BU00iEFcYi_X",
    "outputId": "3dbf33de-9f47-429f-e853-99595b1ade98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_241211_000402.keras\n"
     ]
    }
   ],
   "source": [
    "model = tfk.models.load_model(\n",
    "    model_filename,\n",
    "    custom_objects={\n",
    "        'jaccard_loss': jaccard_loss,\n",
    "        'res_unet_block': res_unet_block\n",
    "    }\n",
    ")\n",
    "print(f\"Model loaded from {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z287uIQ_VGoK",
    "outputId": "b826add0-a1d5-44e3-8faa-ea5fe966b0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m314/314\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 106ms/step\n",
      "Predictions shape: (10022, 64, 128)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_65HpwImM1ts",
    "outputId": "09e042ea-a178-4325-e329-0ce2505a7d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step\n",
      "IoU score for each class (background excluded): [0.7006867530131458, 0.4988689751174526, 0.6321922079005073, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def calculate_iou_per_class(y_true, y_pred, num_classes):\n",
    "    # Remove the channel dimension from y_true if present\n",
    "    y_true = y_true.squeeze()  # or y_true = np.squeeze(y_true, axis=-1)\n",
    "\n",
    "    iou_scores = []\n",
    "    for i in range(num_classes):\n",
    "        intersection = np.logical_and(y_true == i, y_pred == i).sum()\n",
    "        union = np.logical_or(y_true == i, y_pred == i).sum()\n",
    "        if union == 0:\n",
    "            iou_scores.append(np.nan)\n",
    "        else:\n",
    "            iou_scores.append(intersection / union)\n",
    "    return iou_scores\n",
    "preds = model.predict(X_val)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "iou_scores = calculate_iou_per_class(y_val, preds, num_classes)\n",
    "print(\"IoU score for each class (background excluded):\", iou_scores[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KErlLGwOTsX6"
   },
   "source": [
    "## ðŸ’¾ Save the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SPjMEKqZW5jX"
   },
   "outputs": [],
   "source": [
    "def y_to_df(y) -> pd.DataFrame:\n",
    "    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n",
    "    n_samples = len(y)\n",
    "    y_flat = y.reshape(n_samples, -1)\n",
    "    df = pd.DataFrame(y_flat)\n",
    "    df[\"id\"] = np.arange(n_samples)\n",
    "    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "s18kX1uDconq",
    "outputId": "d9cb30ee-758a-49e0-e729-65b390028acc"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_dddf1def-661d-433b-aa1e-03b56f08e2a9\", \"submission_241211_000402.csv\", 164289323)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and download the csv submission file\n",
    "timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n",
    "submission_filename = f\"submission_{timestep_str}.csv\"\n",
    "submission_df = y_to_df(preds)\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "from google.colab import files\n",
    "files.download(submission_filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
